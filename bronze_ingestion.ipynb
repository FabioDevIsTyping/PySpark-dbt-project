{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66c673ec-74a8-4de0-b3d5-d78db4426e0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Simple checks\n",
    "Just using the read function to have an overview of one of the csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6458088c-0d74-4848-95a8-86a44329a57e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .load(\"/Volumes/pysparkdbt/source/source_data/customers/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "258296b5-b27a-4af7-8dd9-a93352e403ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Checking the schema for customers\n",
    "schema_customers = df.schema\n",
    "schema_customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fea6acda-ca61-4285-bbbd-6cf2de5dfc1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "249eefaa-1592-4b79-a975-99e113f15c63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Spark Streaming Process\n",
    "In this process we utilized the readStream function to read the data.\n",
    "We defined entities and obtained the schemas for those entities to make the whole code more dynamic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "508582d8-a519-45f1-8837-05c569857e2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "entities = [\"customers\",\"trips\",\"locations\",\"payments\", \"vehicles\", \"drivers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b7f60bb-ac4c-47e2-adfa-29d3f318fa58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp, col\n",
    "\n",
    "# List of entities to ingest\n",
    "entities = [\"customers\", \"trips\", \"locations\", \"payments\", \"vehicles\", \"drivers\"]\n",
    "\n",
    "queries = []\n",
    "for entity in entities:\n",
    "    # Read a static batch of CSV files once to infer the schema\n",
    "    df_batch = (spark.read.format(\"csv\")\n",
    "                .option(\"header\", \"true\")\n",
    "                .option(\"inferSchema\", \"true\")\n",
    "                .load(f\"/Volumes/pysparkdbt/source/source_data/{entity}/\"))\n",
    "    schema_entity = df_batch.schema\n",
    "\n",
    "    # Define the streaming DataFrame using the schema inferred above\n",
    "    df = (spark.readStream.format(\"csv\")\n",
    "          .option(\"header\", \"true\")\n",
    "          .schema(schema_entity)\n",
    "          .load(f\"/Volumes/pysparkdbt/source/source_data/{entity}/\")\n",
    "          # Add ingestion timestamp (when the row was ingested into the lakehouse)\n",
    "          .withColumn(\"ingestion_timestamp\", current_timestamp())\n",
    "          # Add the source file path (lineage metadata, available in Unity Catalog via _metadata.file_path)\n",
    "          .withColumn(\"input_file_path\", col(\"_metadata.file_path\")))\n",
    "\n",
    "    # Write the stream into a Delta table in the Bronze layer\n",
    "    q = (df.writeStream.format(\"delta\")\n",
    "         # Define a per-entity checkpoint to ensure idempotency and recovery\n",
    "         .option(\"checkpointLocation\", f\"/Volumes/pysparkdbt/bronze/checkpoint/{entity}\")\n",
    "         # Use trigger once to process available data one time and stop\n",
    "         .trigger(once=True)\n",
    "         # Target table in the Bronze schema for this entity\n",
    "         .toTable(f\"pysparkdbt.bronze.{entity}\"))\n",
    "    queries.append(q)\n",
    "\n",
    "# Wait for all queries to finish before exiting\n",
    "for q in queries:\n",
    "    q.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76fabe3f-f1b1-4c59-804f-488dfe7b88ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "/*\n",
    "DROP TABLE IF EXISTS pysparkdbt.bronze.customers;\n",
    "DROP TABLE IF EXISTS pysparkdbt.bronze.trips;\n",
    "DROP TABLE IF EXISTS pysparkdbt.bronze.locations;\n",
    "DROP TABLE IF EXISTS pysparkdbt.bronze.payments;\n",
    "DROP TABLE IF EXISTS pysparkdbt.bronze.vehicles;\n",
    "DROP TABLE IF EXISTS pysparkdbt.bronze.drivers;\n",
    "*/"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5292605373448602,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze_ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
